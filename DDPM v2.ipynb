{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!conda activate ddp\n",
    "!pip install -q -U einops datasets matplotlib tqdm\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "==========\n",
    "\n",
    "https://huggingface.co/blog/annotated-diffusion\n",
    "https://lilianweng.github.io/posts/2021-07-11-diffusion-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mtman/miniconda3/envs/ddpm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from inspect import isfunction\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange, reduce\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Helpers\n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "def default(val,d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    ## Calcs for group normalization\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr=[divisor]*groups\n",
    "    if remainder>0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self,fn):\n",
    "        super().__init__()\n",
    "        self.fn=fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x,*args,**kwargs) + x\n",
    "    \n",
    "def Upsample(dim, dim_out=None):\n",
    "    return nn.Sequential(\n",
    "        nn.Upsample(scale_factor=2, mode='nearest'),nn.Conv2d(dim, default(dim_out,dim),3,padding=1)        \n",
    "    )\n",
    "\n",
    "def Downsample(dim, dim_out=None):\n",
    "    # Assumes image was scaled up with Upsample. Rearrange by splitting out original images and organizing along channel dimension. Then apply 1d convolution. \n",
    "    return nn.Sequential(\n",
    "        Rearrange(\"b c (h p1) (w p2) -> b (c p1 p2) h w\", p1=2, p2=2),\n",
    "        nn.Conv2d(dim * 4, default(dim_out, dim), 1),\n",
    "    )  \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional embeddings\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    # https://arxiv.org/pdf/1706.03762.pdf Numerically stable way to calcuate the sinusoidal position embedding. The motivation for this choice appears to be thinking of the position as projecting the 1-D position value \n",
    "    # into the same dimension space as the model where dimension i corresponds to a frequency component wiht angular frequency of 2*pi*(10000)^(i/(dim/2)) which ranges from 2*pi to 10000*2pi whcih can be thought of as fourier components.\n",
    "    # The authors acknowlege this is arbitrary but argue for it on the basis of this projected co-ordinate system would allow relative positions PE_{pos+k} to be calculated as a linear function of the encoding of PE_{pos+k}\n",
    "\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings) #  exp{log (10^-5)^(i/(dim/2))}\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)  #will be of length dim\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet block: The original paper uses wide resnet blocks but another experimenter replaced the convolution layer with a weighted standardized version which apparently works better with \n",
    "# group normalization https://arxiv.org/abs/1912.11370. Perhaps this is because in group norm we take subsets of channels and perform the operations in parallel. If the states of the groups of channels of G,H,W are different then it \n",
    "# \n",
    "\n",
    "class WeightedStandardizedConv2d(nn.Conv2d):\n",
    "\n",
    "    def forward(self,x):\n",
    "        eps = 1e-5 if x.dtype==torch.float32 else 1e-3\n",
    "        \n",
    "        weight = self.weight\n",
    "        mean = reduce(weight, \"o ... -> o 1 1 1\", \"mean\") # mean over each of the RGB channels. Keep dims\n",
    "        var = reduce(weight, \"o ... -> o 1 1 1\", partial(torch.var, unbiased=False)) # var over each of the RGB chanells. Keep dims\n",
    "        normalized_weight = (weight - mean) * (var + eps).rsqrt() # rsqrt (reciprocal sqrt ... really :) ) \n",
    "\n",
    "        return F.conv2d(\n",
    "            x,\n",
    "            normalized_weight,\n",
    "            self.bias,\n",
    "            self.stride,\n",
    "            self.padding,\n",
    "            self.dilation,\n",
    "            self.groups,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out, groups=8):\n",
    "        super().__init__()\n",
    "        self.proj = WeightedStandardizedConv2d(dim,dim_out,3, padding=1)  #\n",
    "        self.norm=nn.GroupNorm(groups, dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "    \n",
    "    def forward(self, x, scale_shift=None):\n",
    "        x=self.proj(x)\n",
    "        x=self.norm(x)\n",
    "        if exists(scale_shift):\n",
    "            #scale and shift terms come from the fourier transform interpretation of the sinusoidal position embedding. F(w) = \\int_-inf^inf f(t).exp(-iwt)dt. let f(t)->f(t-\\del t) => exp(-iw\\del t)F(W) phase shift.\n",
    "            scale, shift = scale_shift\n",
    "            x=x*(scale+1)+shift\n",
    "        x=self.act(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n",
    "        super().__init__()\n",
    "        self.mlp=nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out*2)) if exists(time_emb_dim) else None\n",
    "\n",
    "        self.block1=Block(dim, dim_out, groups=groups)\n",
    "        self.block2=Block(dim_out,dim_out,groups=groups)\n",
    "        self.res_conv=nn.Conv2d(dim, dim_out,1) if dim!=dim_out else nn.Identity() # Either skip connection if input and output dimensions are the same otherwise a 1x1 convolution\n",
    "\n",
    "    def forward(self,x, time_emb=None):\n",
    "        scale_shift= None\n",
    "        if exists(self.mlp) and exists(time_emb):\n",
    "            time_emb=self.mlp(time_emb)\n",
    "            time_emb=rearrange(time_emb, \"b c -> b c 1 1\")\n",
    "            scale_shift=time_emb.chunk(2,dim=1)   # (sin , cos) of time encoding\n",
    "        h=self.block1(x,scale_shift=scale_shift)\n",
    "        h=self.block2(h)\n",
    "        return h+self.res_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False) # 1x1 convolution used to create random projections for Q K and V from the input image.\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Attention is applied between convolution layers in the Unet arch. Each channel is the results of different convolution filters so can be thought of different embeddings of the visual tokens. \n",
    "        # In Multihead attention each head represents a different 'reference plane' whcih in the NLP context represent different embedding subspaces and it is hoped that each captures different aspects of the semantics. \n",
    "        # By analoge, multihead attention in image context, each channel the projection of the image onto a different subspace (via convolution filters) with the hope that each captures different semantics. The variation is that\n",
    "        # in NLP each head deals with a sequence of token embeddings, in the current paradigm for handling images each channel corresponds to one token and output convolution is the embedding of that token. \n",
    "        # \n",
    "        b, c, h, w = x.shape\n",
    "        qkv=self.to_qkv(x).chunk(3,dim=1)   # split the composite qkv created from the random projections ( 1x1 convolutions) . Returns list of 3 tensors.\n",
    "        q,k,v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)',h=self.heads),qkv) #split out q k v for each head h, and unroll the 2d projections of the image \n",
    "\n",
    "        q=q*self.scale\n",
    "        # sim = einsum('b h d i, b h d j -> b h i j', q, k) # inner product along the channel dimension gives the i,j-th 'pixel' similarity across \n",
    "        sim = einsum('b h i e, b h j e -> b h i j', q, k) # inner product along the channel dimension gives the i,j-th 'pixel' similarity across \n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        attn=sim.softmax(dim=-1)\n",
    "\n",
    "        out = einsum(\"b h i j, b h j d -> b h i d\", attn, v) # this was wrong in the blog. \n",
    "        out = rearrange(out, \"b h d (x y) -> b (h d) x y\", x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "    \n",
    "class LinearAttention(nn.Module):\n",
    "    '''https://arxiv.org/ftp/arxiv/papers/2007/2007.14902.pdf\n",
    "        Linear attention idea development starts with characterising the softmax matmult of QK with V as a specific instance of kernel operation. The choose first order taylor series approximatino to the softmax to get a linear attention mechanism\n",
    "        that is computationally more efficient.\n",
    "\n",
    "    '''\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1), \n",
    "                                    nn.GroupNorm(1, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "\n",
    "        q = q.softmax(dim=-2)\n",
    "        k = k.softmax(dim=-1)\n",
    "\n",
    "        q = q * self.scale\n",
    "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
    "\n",
    "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
    "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        # Groupnorm with 1 group is layer norm\n",
    "        self.norm = nn.GroupNorm(1, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Conditional Unet.\n",
    "'''\n",
    "class Unet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            init_dim=None,\n",
    "            out_dim=None,\n",
    "            dim_mults=(1,2,4,8),\n",
    "            channels=3,\n",
    "            self_condition=False,\n",
    "            resnet_block_groups=4\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels = channels # starts at RGB\n",
    "        self.self_condition = self_condition\n",
    "        input_channels = channels* (2 if self_condition else 1)\n",
    "\n",
    "        init_dim = default(init_dim, dim)\n",
    "        self.init_conv = nn.Conv2d(input_channels, init_dim, 1 , padding =0) # first layer of Unet. Number of filters ( init_dim) \n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)] # init_dim defaults to dim if no value passed in. \n",
    "        in_out = list(zip(dims[:-1], dims[1:])) # the input to output dimension changes for downsample, upsample\n",
    "\n",
    "        block_klass = partial(ResnetBlock, groups = resnet_block_groups)  # resnet_block_groups is for group norm in resnetblock. Values used are 4 and 8. Note this works in conjunction with normalised weight conv2d\n",
    "\n",
    "        # The FCN in transformer block do a 4 x dim expansion and then a projection. This is applying the same operation to time embedding. \n",
    "        time_dim = dim*4 \n",
    "        self.time_mlp = nn.Sequential(SinusoidalPositionEmbeddings(dim),\n",
    "                                      nn.Linear(dim, time_dim),\n",
    "                                      nn.GELU(),\n",
    "                                      nn.Linear(time_dim, time_dim),)\n",
    "        \n",
    "        self.downs=nn.ModuleList([]) # registers all modules in list for __all__\n",
    "        self.ups=nn.ModuleList([])\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind>=(num_resolutions-1)\n",
    "            self.downs.append(nn.ModuleList([\n",
    "                block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                Downsample(dim_in, dim_out)\n",
    "                if not is_last else nn.Conv2d(dim_in, dim_out, 3, padding =1),\n",
    "            ]))\n",
    "        \n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
    "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
    "            is_last = ind == (len(in_out) - 1)\n",
    "\n",
    "            self.ups.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                        block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                        Upsample(dim_out, dim_in)\n",
    "                        if not is_last\n",
    "                        else nn.Conv2d(dim_out, dim_in, 3, padding=1),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.out_dim = default(out_dim, channels)\n",
    "\n",
    "        self.final_res_block = block_klass(dim * 2, dim, time_emb_dim=time_dim)\n",
    "        self.final_conv = nn.Conv2d(dim, self.out_dim, 1)\n",
    "\n",
    "    def forward(self, x, time, x_self_cond=None):\n",
    "        if self.self_condition:\n",
    "            x_self_cond = default(x_self_cond, lambda: torch.zeros_like(x))\n",
    "            x = torch.cat((x_self_cond, x), dim=1)\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "        r = x.clone() \n",
    "\n",
    "        t = self.time_mlp(time)  # time is passed through 2 MLP's and not sure why? \n",
    "\n",
    "        h = []\n",
    "\n",
    "        for block1, block2, attn, downsample in self.downs:\n",
    "            x = block1(x, t)\n",
    "            h.append(x)\n",
    "\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "\n",
    "            x = downsample(x)\n",
    "\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t)\n",
    "\n",
    "        for block1, block2, attn, upsample in self.ups:\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block1(x, t)\n",
    "\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = torch.cat((x, r), dim=1)\n",
    "\n",
    "        x = self.final_res_block(x, t)\n",
    "        return self.final_conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unet(\n",
       "  (init_conv): Conv2d(3, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (time_mlp): Sequential(\n",
       "    (0): SinusoidalPositionEmbeddings()\n",
       "    (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (downs): ModuleList(\n",
       "    (0): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): WeightedStandardizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 128, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): WeightedStandardizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 128, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Identity()\n",
       "      )\n",
       "      (2): Residual(\n",
       "        (fn): PreNorm(\n",
       "          (fn): LinearAttention(\n",
       "            (to_qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "            )\n",
       "          )\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
       "        (1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (1): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): WeightedStandardizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 128, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): WeightedStandardizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 128, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Identity()\n",
       "      )\n",
       "      (2): Residual(\n",
       "        (fn): PreNorm(\n",
       "          (fn): LinearAttention(\n",
       "            (to_qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "            )\n",
       "          )\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (2): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): WeightedStandardizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 256, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): WeightedStandardizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 256, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Identity()\n",
       "      )\n",
       "      (2): Residual(\n",
       "        (fn): PreNorm(\n",
       "          (fn): LinearAttention(\n",
       "            (to_qkv): Conv2d(256, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "            )\n",
       "          )\n",
       "          (norm): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (ups): ModuleList(\n",
       "    (0): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): WeightedStandardizedConv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 512, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): WeightedStandardizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 512, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): Residual(\n",
       "        (fn): PreNorm(\n",
       "          (fn): LinearAttention(\n",
       "            (to_qkv): Conv2d(512, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "            )\n",
       "          )\n",
       "          (norm): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Upsample(scale_factor=2.0, mode='nearest')\n",
       "        (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (1): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): WeightedStandardizedConv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 256, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): WeightedStandardizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 256, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): Residual(\n",
       "        (fn): PreNorm(\n",
       "          (fn): LinearAttention(\n",
       "            (to_qkv): Conv2d(256, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "            )\n",
       "          )\n",
       "          (norm): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Upsample(scale_factor=2.0, mode='nearest')\n",
       "        (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (2): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): WeightedStandardizedConv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 128, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): WeightedStandardizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(4, 128, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): Residual(\n",
       "        (fn): PreNorm(\n",
       "          (fn): LinearAttention(\n",
       "            (to_qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "            )\n",
       "          )\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (mid_block1): ResnetBlock(\n",
       "    (mlp): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    )\n",
       "    (block1): Block(\n",
       "      (proj): WeightedStandardizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm): GroupNorm(4, 512, eps=1e-05, affine=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (block2): Block(\n",
       "      (proj): WeightedStandardizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm): GroupNorm(4, 512, eps=1e-05, affine=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (res_conv): Identity()\n",
       "  )\n",
       "  (mid_attn): Residual(\n",
       "    (fn): PreNorm(\n",
       "      (fn): Attention(\n",
       "        (to_qkv): Conv2d(512, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (to_out): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (norm): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (mid_block2): ResnetBlock(\n",
       "    (mlp): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    )\n",
       "    (block1): Block(\n",
       "      (proj): WeightedStandardizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm): GroupNorm(4, 512, eps=1e-05, affine=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (block2): Block(\n",
       "      (proj): WeightedStandardizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm): GroupNorm(4, 512, eps=1e-05, affine=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (res_conv): Identity()\n",
       "  )\n",
       "  (final_res_block): ResnetBlock(\n",
       "    (mlp): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "    (block1): Block(\n",
       "      (proj): WeightedStandardizedConv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm): GroupNorm(4, 128, eps=1e-05, affine=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (block2): Block(\n",
       "      (proj): WeightedStandardizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (norm): GroupNorm(4, 128, eps=1e-05, affine=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (res_conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (final_conv): Conv2d(128, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet = Unet(dim=image_size,channels =3, dim_mults=(1,2,4,))\n",
    "unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule as proposed in https://arxiv.org/abs/2102.09672\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "def quadratic_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2\n",
    "\n",
    "def sigmoid_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    betas = torch.linspace(-6, 6, timesteps)\n",
    "    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers to deal with time step\n",
    "timesteps=1000 # T=300 for fashion mnist\n",
    "\n",
    "#create linear schedule of betas for this demo.\n",
    "betas = linear_beta_schedule(timesteps=timesteps)\n",
    "\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod=torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1,0), value=1.0) #prepend a 1. to the shifted list of cumprod alphas\n",
    "sqrt_recip_alphas = torch.sqrt(1.0/alphas)\n",
    "\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod=torch.sqrt(1. - alphas_cumprod)  # see equation (11) is paper. $$ \\mu_{\\theta}(x_t,t) = {1 \\over \\sqrt{\\alpha_{t}}}\\left(x_t - {\\beta_t \\over \\sqrt{1-\\bar{\\alpha_t}}} \\epsilon_\\theta(x_t, t)\\right)$$\n",
    "\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev)/(1. - alphas_cumprod)\n",
    "\n",
    "def extract(a,t,x_shape):\n",
    "    '''Index into the alpha's, betas etc (a) and shape it to be able to apply it pixelwise to the image tensor.\n",
    "    a = cummulative alphas, beta etc.\n",
    "    t = time index into a.\n",
    "    x_shape : shape of input tensor (b) x ....   \n",
    "    '''\n",
    "    batch_size=t.shape[0]\n",
    "    out=a.gather(-1,t.cpu())\n",
    "    return out.reshape(batch_size, *((1,)*(len(x_shape)-1))).to(t.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, forward diffusion as per paper\n",
    "def q_sample(x_start: torch.tensor, t: float, noise=None):\n",
    "    if noise is None:\n",
    "        noise=torch.randn_like(x_start)\n",
    "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
    "    )\n",
    "    return sqrt_alphas_cumprod_t*x_start + sqrt_one_minus_alphas_cumprod_t* noise  # see the simplified training objective for the noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def plot(imgs, with_orig=False, row_title=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0]) + with_orig\n",
    "    fig, axs = plt.subplots(figsize=(128,128), nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        #row = [image] + row if with_orig else row\n",
    "        \n",
    "        for col_idx, img in enumerate(row):\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if with_orig:\n",
    "        axs[0, 0].set(title='Original image')\n",
    "        axs[0, 0].title.set_size(8)\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_losses(denoise_model, x_start, t, noise=None, loss_type=\"l1\"):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)  # The beauty of the diffusion model is that the forward diffusion process has a closed form expression. \n",
    "    predicted_noise = denoise_model(x_noisy, t)\n",
    "\n",
    "    if loss_type == 'l1':\n",
    "        loss = F.l1_loss(noise, predicted_noise)\n",
    "    elif loss_type == 'l2':\n",
    "        loss = F.mse_loss(noise, predicted_noise)\n",
    "    elif loss_type == \"huber\":\n",
    "        loss = F.smooth_l1_loss(noise, predicted_noise)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def p_sample(model, x, t, t_index):\n",
    "    '''Reverse process\n",
    "    returns: Tensor\n",
    "    '''\n",
    "    betas_t = extract(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "    )\n",
    "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
    "    \n",
    "    # Equation 11 in the paper\n",
    "    # Use our model (noise predictor) to predict the mean\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "\n",
    "    if t_index == 0:\n",
    "        return model_mean\n",
    "    else:\n",
    "        posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
    "        noise = torch.randn_like(x)\n",
    "        # Algorithm 2 line 4:\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise  # x_{t-1}\n",
    "\n",
    "# Algorithm 2 (including returning all images)\n",
    "@torch.no_grad()\n",
    "def p_sample_loop(model, shape):\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    b = shape[0]\n",
    "    # start from pure noise (for each example in the batch)\n",
    "    img = torch.randn(shape, device=device)\n",
    "    imgs = []\n",
    "\n",
    "    for i in tqdm(reversed(range(0, timesteps)), desc='sampling loop time step', total=timesteps):\n",
    "        img = p_sample(model, img, torch.full((b,), i, device=device, dtype=torch.long), i)\n",
    "        #imgs.append(img.cpu().numpy())\n",
    "        imgs.append(img.cpu())\n",
    "    return imgs\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, image_size, batch_size=16, channels=3):\n",
    "    '''\n",
    "    model: denoising model\n",
    "    images_size: int: images assumed square, pixel length of side.\n",
    "    batch: number of samples to propogate back through the diffusion process.\n",
    "    @returns:\n",
    "    list of batches of tensors: '''\n",
    "    return p_sample_loop(model, shape=(batch_size, channels, image_size, image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "results_folder = Path(\"./results\")\n",
    "results_folder.mkdir(exist_ok = True)\n",
    "save_and_sample_every = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202599, 1)\n"
     ]
    }
   ],
   "source": [
    "# Celeba dataloader\n",
    "\n",
    "image_root_dir='./img_align_celeba/img_align_celeba/'\n",
    "image_file_list ='./img_align_celeba_file.csv'\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "image_size=128\n",
    "channels=3\n",
    "batch_size=24\n",
    "\n",
    "class CelebDataSet(Dataset):\n",
    "    def __init__(self,image_list, image_root_path, transform=None):\n",
    "\n",
    "        self.image_list = pd.read_csv(image_file_list,names=['filename'])\n",
    "        print(self.image_list.shape)\n",
    "        self.image_files = self.image_list['filename'].apply(lambda x: os.path.normpath(image_root_path+os.sep+f'{x}'))\n",
    "        self.transforms = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.image_list.shape[0] # includes corrupted images\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx>=len(self.image_list):\n",
    "            raise RuntimeError('image index out of range')\n",
    "        ret=None\n",
    "        try:\n",
    "            img = Image.open(self.image_files[idx])\n",
    "            if self.transforms:\n",
    "                ret = self.transforms(img)\n",
    "            else:\n",
    "                raise RuntimeError('PILimages must be transformed into tensors and pixel values rescaled to range [-1,1]')\n",
    "        except Exception as e:\n",
    "            print(f'error reading image with index {idx}')\n",
    "            print(f'{e}')\n",
    "        return ret, idx\n",
    "\n",
    "# define image transformations (e.g. using torchvision)\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(image_size),\n",
    "            transforms.CenterCrop(image_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),   # scales by 255\n",
    "            transforms.Lambda(lambda t: (t * 2) - 1)\n",
    "])\n",
    "\n",
    "dataset = CelebDataSet(image_file_list,image_root_path=image_root_dir, transform=transform)\n",
    "dataloader=DataLoader(dataset,batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.transforms import Lambda,Compose,Resize,ToPILImage\n",
    "tensor_to_PIL = Compose([\n",
    "    Lambda(lambda t: (t+1)/2),\n",
    "    Lambda(lambda t: t.permute(1,2,0)), #CHW to HWC\n",
    "    Lambda(lambda t: t*255.)\n",
    "    ,Lambda(lambda t:t.numpy().astype(np.uint8)),  # Automaticallly clips it to 255\n",
    "    ToPILImage(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = Unet(\n",
    "    dim=image_size,\n",
    "    channels=channels,\n",
    "    dim_mults=(1, 2, 4, 8)\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup wandb\n",
    "import wandb\n",
    "from uuid import uuid4\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(host='http://localhost:8080', key='local-c6445bd877c8f3cec4f5abdaa31e4266d042408e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:v2rqlu0o) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run-54d5e88</strong> at: <a href='http://localhost:8080/mattma1970/ddpm-caleba/runs/v2rqlu0o' target=\"_blank\">http://localhost:8080/mattma1970/ddpm-caleba/runs/v2rqlu0o</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230726_034500-v2rqlu0o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:v2rqlu0o). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mtman/Documents/Repos/denoising-diffusion-pytorch/wandb/run-20230726_034857-p1b4mnef</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/mattma1970/ddpm-caleba/runs/p1b4mnef' target=\"_blank\">run-28653d3</a></strong> to <a href='http://localhost:8080/mattma1970/ddpm-caleba' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/mattma1970/ddpm-caleba' target=\"_blank\">http://localhost:8080/mattma1970/ddpm-caleba</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/mattma1970/ddpm-caleba/runs/p1b4mnef' target=\"_blank\">http://localhost:8080/mattma1970/ddpm-caleba/runs/p1b4mnef</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#wandb.login()\n",
    "hash = str(uuid4())[:7]\n",
    "\n",
    "run=wandb.init(\n",
    "                project=f'ddpm-caleba',\n",
    "                name=f'run-{hash}',\n",
    "                config={\n",
    "                    'timesteps':timesteps,\n",
    "                    'lr':optimizer.param_groups[0]['lr']\n",
    "               })\n",
    "\n",
    "checkpoint_path = f'./checkpoints/{hash}'\n",
    "\n",
    "os.makedirs(checkpoint_path, exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "def show_sample(model, image_size, batch_size=32,*, save_only=False, epoch=-1, step=-1, hash=None):\n",
    "    samples = sample(model, image_size=image_size, batch_size=batch_size, channels=3) # returns lsit of batches of samples.\n",
    "    # show a random one\n",
    "    random_index = 2\n",
    "    a=tensor_to_PIL(samples[-1][random_index,:,:,:].squeeze()) # convert to PILImage\n",
    "    if save_only:\n",
    "        a.save(f'{results_folder}/{hash}-{epoch}-{step}.png')\n",
    "        #save_image(all_images, str(results_folder / f'sample-{epoch}.png'), nrow = 1)\n",
    "    else:\n",
    "        a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDPM training loop:   0%|          | 0/8442 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, batch#:0, Loss:0.4004521369934082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [01:42<00:00,  9.72it/s]\n",
      "DDPM training loop:   1%|          | 100/8442 [02:28<1:03:14,  2.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, batch#:100, Loss:0.02693931572139263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from torchvision.utils import save_image\n",
    "from itertools import chain\n",
    "\n",
    "timesteps=1000\n",
    "epochs = 4\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print (f'Epoch {epoch}')\n",
    "    for step, batch in enumerate(tqdm(dataloader, desc='DDPM training loop')):\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      batch_size = batch[0].shape[0]\n",
    "      batch = batch[0].to(device)\n",
    "\n",
    "      # Algorithm 1 line 3: sample t uniformally for every example in the batch\n",
    "      t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
    "\n",
    "      loss = p_losses(model, batch, t, loss_type=\"huber\")\n",
    "  \n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      if step % 100 == 0:\n",
    "        print(f\"Epoch:{epoch}, batch#:{step}, Loss:{loss.item()}\")\n",
    "        wandb.log({'loss':loss})\n",
    "      if step %100 ==0:\n",
    "         show_sample(model,image_size,16, save_only=True, epoch=epoch, step=step, hash=hash)\n",
    "    # save generated images\n",
    "    #if step != 0 and step % save_and_sample_every == 0:\n",
    "    #if step % save_and_sample_every == 0:\n",
    "    #export some samples\n",
    "    batches = num_to_groups(4, batch_size)\n",
    "    all_images_list = list(map(lambda n: sample(model, image_size=image_size, batch_size=n, channels=channels), batches))\n",
    "    # flatten the map nested lists\n",
    "    all_images_list = list(chain.from_iterable(all_images_list))\n",
    "    all_images = torch.cat(all_images_list, dim=0)\n",
    "    all_images = (all_images + 1) * 0.5\n",
    "    save_image(all_images, str(results_folder / f'sample-{epoch}.png'), nrow = 1)\n",
    "    \n",
    "    torch.save({'epoch':epoch,\n",
    "          'model_state_dict':model.state_dict(),\n",
    "          'optimizer_state_dict':optimizer.state_dict(),\n",
    "          'loss':loss}, f'./checkpoints/celeba-model-{epoch}.pt')\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('./checkpoints/celeba-model-0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample(model, image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "\n",
    "random_index = 2\n",
    "\n",
    "fig = plt.figure()\n",
    "ims = []\n",
    "for i in range(timesteps):\n",
    "    im = plt.imshow(samples[i][random_index].reshape(image_size, image_size, channels), cmap=\"gray\", animated=True)\n",
    "    ims.append([im])\n",
    "\n",
    "animate = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=1000)\n",
    "animate.save('diffusion.gif')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddpm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
